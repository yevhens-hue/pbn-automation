import json
import os
import csv
import requests
import sqlite3
from datetime import datetime
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

def init_db():
    """
    Initializes the SQLite database with the requested schema.
    """
    try:
        conn = sqlite3.connect('pbn_metrics.db')
        c = conn.cursor()
        # –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è –æ–±—â–µ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        c.execute('''CREATE TABLE IF NOT EXISTS publications
                     (id INTEGER PRIMARY KEY AUTOINCREMENT,
                      timestamp DATETIME,
                      success_count INTEGER,
                      error_count INTEGER,
                      links_inserted INTEGER,
                      cost_usd REAL)''')
        # –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –ø–æ —Å—Ç–∏–ª—è–º
        c.execute('''CREATE TABLE IF NOT EXISTS persona_stats
                     (persona TEXT,
                      posts_count INTEGER,
                      avg_length INTEGER)''')
        conn.commit()
        conn.close()
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –ë–î: {e}")

def log_to_grafana(success, errors, links, cost, style_stats=None):
    """
    Logs metrics to SQLite database for Grafana visualization.
    """
    try:
        init_db()
        conn = sqlite3.connect('pbn_metrics.db')
        c = conn.cursor()
        
        # Log to publications table
        c.execute("INSERT INTO publications (timestamp, success_count, error_count, links_inserted, cost_usd) VALUES (?, ?, ?, ?, ?)",
                  (datetime.now(), success, errors, links, cost))
        
        # Log to persona_stats table
        if style_stats:
            # Clear old stats or update? The user's prompt suggests a cumulative or snapshot view. 
            # We'll insert current batch stats.
            for style, data in style_stats.items():
                avg_len = data['total_len'] / data['count'] if data['count'] > 0 else 0
                c.execute("INSERT INTO persona_stats (persona, posts_count, avg_length) VALUES (?, ?, ?)",
                          (style, data['count'], int(avg_len)))
        
        conn.commit()
        conn.close()
        print("üìä –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω—ã —Å SQLite (pbn_metrics.db)")
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ –≤ –ë–î: {e}")

def calculate_dashboard_metrics(results_file='results.json', logs_file='generation_logs.jsonl'):
    print("=== PBN Executive Dashboard ===")
    
    # 1. Load Results
    if not os.path.exists(results_file):
        print(f"Error: {results_file} not found. Run publishing first.")
        return
    
    with open(results_file, 'r') as f:
        results = json.load(f)
    
    total_tasks = len(results)
    success_count = sum(1 for r in results if r.get('status') == 'success')
    error_count = total_tasks - success_count
    link_injection_count = sum(1 for r in results if r.get('updated_old_post'))
    new_posts_count = success_count # assuming each success is a new post or update
    
    print(f"\n[Overall Performance]")
    print(f"Total Sites: {total_tasks}")
    print(f"Successful:  {success_count} | Errors: {error_count}")
    print(f"Old Updated: {link_injection_count} | New Created: {new_posts_count}")

    # Generate execution_summary.csv
    with open('execution_summary.csv', 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(['Metric', 'Value', 'Description'])
        writer.writerow(['Total Sites', total_tasks, '–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∞–π—Ç–æ–≤ –≤ –±–∞–∑–µ'])
        writer.writerow(['Successful Posts', success_count, '–û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ –±–µ–∑ –æ—à–∏–±–æ–∫'])
        writer.writerow(['Errors', error_count, '–ü—Ä–æ–±–ª–µ–º—ã (—Ç–∞–π–º–∞—É—Ç—ã, –Ω–µ–≤–µ—Ä–Ω—ã–µ –ø–∞—Ä–æ–ª–∏)'])
        writer.writerow(['Old Posts Updated', link_injection_count, '–°—Å—ã–ª–æ–∫ –¥–æ–±–∞–≤–ª–µ–Ω–æ –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç'])
        writer.writerow(['New Posts Created', new_posts_count, '–°–æ–∑–¥–∞–Ω–æ –Ω–æ–≤—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü —Å –Ω—É–ª—è'])

    # 2. Analyze Styles (Persona)
    style_stats = {}
    if os.path.exists(logs_file):
        with open(logs_file, 'r') as f:
            for line in f:
                try:
                    log = json.loads(line)
                    style = log.get('style', 'unknown')
                    content_len = len(log.get('response', ''))
                    
                    if style not in style_stats:
                        style_stats[style] = {'count': 0, 'total_len': 0, 'indexed': 0}
                    style_stats[style]['count'] += 1
                    style_stats[style]['total_len'] += content_len
                    # For demo purposes, we simulate an indexing rate if not tracked
                    # In a real app, verify_posts.py would update this.
                except:
                    continue

        print(f"\n[Persona & Content Metrics]")
        with open('persona_analytics.csv', 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow(['Persona', 'Posts', 'Indexing Rate (%)', 'Avg Length (chars)'])
            
            for style, data in style_stats.items():
                avg_len = data['total_len'] / data['count']
                # Dummy indexing rate for visualization as requested
                indexing_rate = 91 if style == 'lifestyle' else (82 if style == 'expert' else 75)
                
                print(f"Style: {style:10} | Posts: {data['count']:3} | Avg Length: {avg_len:4.0f} chars")
                writer.writerow([style.capitalize(), data['count'], f"{indexing_rate}%", int(avg_len)])

    # 3. Economical Metrics (Gemini 1.5 Flash approx)
    total_chars = sum(s['total_len'] for s in style_stats.values()) if style_stats else 0
    tokens = total_chars / 2 
    estimated_cost = (tokens / 1_000_000) * 0.075
    
    print(f"\n[Economical Metrics]")
    print(f"Estimated Gemini Cost: ${estimated_cost:.4f}")
    print("\nCSV Reports generated: execution_summary.csv, persona_analytics.csv")
    print("===============================")
    
    # Save to SQLite for Grafana using the new log_to_grafana function
    log_to_grafana(success_count, error_count, link_injection_count, estimated_cost, style_stats)

def send_telegram_report(summary_file):
    """
    Sends a summary report to Telegram if credentials are set in .env.
    """
    token = os.getenv("TELEGRAM_BOT_TOKEN")
    chat_id = os.getenv("TELEGRAM_CHAT_ID")
    
    if not token or not chat_id:
        print("\n‚ÑπÔ∏è  Telegram credentials not set. Skipping report.")
        return

    try:
        # Read summary data
        summary_text = ""
        with open(summary_file, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                summary_text += f"‚Ä¢ {row['Metric']}: *{row['Value']}*\n"

        message = f"üìä *PBN Daily Report* ({datetime.now().strftime('%Y-%m-%d')})\n\n{summary_text}"
        url = f"https://api.telegram.org/bot{token}/sendMessage"
        
        response = requests.post(url, json={
            "chat_id": chat_id,
            "text": message,
            "parse_mode": "Markdown"
        }, timeout=10)
        
        if response.status_code == 200:
            print("‚úÖ –û—Ç—á–µ—Ç —É—Å–ø–µ—à–Ω–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω –≤ Telegram!")
        else:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ Telegram: {response.text}")
            
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ –æ—Ç—á–µ—Ç–∞ Telegram: {e}")

if __name__ == "__main__":
    calculate_dashboard_metrics()
    send_telegram_report('execution_summary.csv')
